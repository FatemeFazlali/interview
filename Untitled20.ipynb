{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/olbY+85V3Wp5mI6wbjwS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatemeFazlali/interview/blob/main/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establish connection to PostgreSQL"
      ],
      "metadata": {
        "id": "8TnwY4Gr7WZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2-binary\n",
        "import psycopg2\n",
        "\n",
        "# Establish connection to PostgreSQL\n",
        "try:\n",
        "    conn = psycopg2.connect(\n",
        "        dbname=\"your_database_name\",\n",
        "        user=\"your_username\",\n",
        "        password=\"your_password\",\n",
        "        host=\"your_host\",\n",
        "        port=\"your_port\"\n",
        "    )\n",
        "    cursor = conn.cursor()\n",
        "    print(\"Connection successful!\")\n",
        "except psycopg2.Error as e:\n",
        "    print(\"Error connecting to database:\", e)\n",
        "\n",
        "\n",
        "# Fetch data from PostgreSQL table\n",
        "query = \"SELECT * FROM table_name;\"\n",
        "cursor.execute(query)\n",
        "data = cursor.fetchall()\n",
        "\n",
        "# Convert data to a pandas DataFrame for exploration and cleaning\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data, columns=['column1', 'column2', ...])  # Replace with column names\n",
        "\n",
        "# Explore the data (e.g., display first few rows, data types, summary statistics)\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "QWGX-XJN7QJn",
        "outputId": "96e42f04-03a6-4689-9207-bdc54150198d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (2.9.9)\n",
            "Error connecting to database: invalid integer value \"your_port\" for connection option \"port\"\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f6a5563b281e>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Fetch data from PostgreSQL table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SELECT * FROM your_table_name;\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cursor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Accessing Data from Google Drive"
      ],
      "metadata": {
        "id": "HHDBMP0dqah3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZJkoBL6eFjzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1787b267-e389-4b88-d361-771c1b6fe970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.10/dist-packages (4.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install rarfile\n",
        "\n",
        "from google.colab import drive\n",
        "import rarfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the RAR file in Google Drive\n",
        "rar_path = '/content/drive/My Drive/interview/Copy of RB_DishwashWeekly_GB.rar'\n",
        "\n",
        "# Destination folder path to extract\n",
        "extract_folder_path = '/content/data'  # Destination folder to extract\n",
        "\n",
        "# Ensure the extraction folder exists, create if it doesn't\n",
        "if not os.path.exists(extract_folder_path):\n",
        "    os.makedirs(extract_folder_path)\n",
        "\n",
        "# Extract the RAR file\n",
        "with rarfile.RarFile(rar_path, 'r') as rf:\n",
        "    rf.extractall(extract_folder_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_FCT = pd.read_csv('/content/data/RB_DishwashWeekly_GB_FCT_07072021.csv', delimiter='|')\n",
        "# Drop columns where all values are NaN or None\n",
        "df_FCT.dropna(axis=1, how='all', inplace=True)\n",
        "print(df_FCT.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559f2412-36c6-4123-f089-875978ec89a0",
        "id": "PXzWDw4mVT0D"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 124 entries, 0 to 123\n",
            "Data columns (total 7 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   TAG            124 non-null    object\n",
            " 1   SHORT          124 non-null    object\n",
            " 2   LONG           124 non-null    object\n",
            " 3   DISPLAY_ORDER  124 non-null    int64 \n",
            " 4   CURRENCY       124 non-null    object\n",
            " 5   PRECISION      124 non-null    int64 \n",
            " 6   DENOMINATOR    124 non-null    int64 \n",
            "dtypes: int64(3), object(4)\n",
            "memory usage: 6.9+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV file path\n",
        "csv_file_path = '/content/data/RB_DishwashWeekly_GB_fact_data_07072021.csv'\n",
        "\n",
        "# Read the first few rows to inspect the data types\n",
        "chunk_size = 10000  # Adjust this chunk size as needed\n",
        "df_chunk = pd.read_csv(csv_file_path, delimiter='|', nrows=chunk_size)\n",
        "\n",
        "# Display data types and other information\n",
        "print(df_chunk.info())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxVrho9mwFEw",
        "outputId": "3f255c86-3982-4090-deb0-8e18fd1edbf5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Columns: 127 entries, MKT_TAG to F000000000000000521900000000000000031002\n",
            "dtypes: float64(123), int64(1), object(3)\n",
            "memory usage: 9.7+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_FCT = pd.read_csv('/content/data/RB_DishwashWeekly_GB_FCT_07072021.csv', delimiter='|')\n",
        "# Drop columns where all values are NaN or None\n",
        "df_FCT.dropna(axis=1, how='all', inplace=True)\n",
        "print(df_FCT.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1GSvwse-pn3",
        "outputId": "f36710c2-d53b-4cd3-be82-f32183d4e2f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 124 entries, 0 to 123\n",
            "Data columns (total 7 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   TAG            124 non-null    object\n",
            " 1   SHORT          124 non-null    object\n",
            " 2   LONG           124 non-null    object\n",
            " 3   DISPLAY_ORDER  124 non-null    int64 \n",
            " 4   CURRENCY       124 non-null    object\n",
            " 5   PRECISION      124 non-null    int64 \n",
            " 6   DENOMINATOR    124 non-null    int64 \n",
            "dtypes: int64(3), object(4)\n",
            "memory usage: 6.9+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_MKT = pd.read_csv('/content/data/RB_DishwashWeekly_GB_MKT_07072021.csv', delimiter='|')\n",
        "df_MKT.dropna(axis=1, how='all', inplace=True)\n",
        "print(df_MKT.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-K4dWNJ-q7L",
        "outputId": "e220e804-ba44-4106-f306-a5a91c94a434"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 38 entries, 0 to 37\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   TAG              38 non-null     object\n",
            " 1   SHORT            38 non-null     object\n",
            " 2   LONG             38 non-null     object\n",
            " 3   DISPLAY_ORDER    38 non-null     int64 \n",
            " 4   PARENT_TAG       30 non-null     object\n",
            " 5   HIER_NUM         38 non-null     int64 \n",
            " 6   HIER_NAME        38 non-null     object\n",
            " 7   HIER_LEVEL_NUM   38 non-null     int64 \n",
            " 8   HIER_LEVEL_NAME  38 non-null     object\n",
            "dtypes: int64(3), object(6)\n",
            "memory usage: 2.8+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_PER = pd.read_csv('/content/data/RB_DishwashWeekly_GB_PER_07072021.csv', delimiter='|')\n",
        "df_PER.dropna(axis=1, how='all', inplace=True)\n",
        "print(df_PER.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rViWpj--t6B",
        "outputId": "374ee72b-0ee9-4503-ecbe-99805363f7c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160 entries, 0 to 159\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   TAG            160 non-null    object\n",
            " 1   SHORT          160 non-null    object\n",
            " 2   LONG           160 non-null    object\n",
            " 3   DISPLAY_ORDER  160 non-null    int64 \n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 5.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_PROD = pd.read_csv('/content/data/RB_DishwashWeekly_GB_PROD_07072021.csv', delimiter='|')\n",
        "df_PROD.dropna(axis=1, how='all', inplace=True)\n",
        "print(df_PROD.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnZruOlc9fAd",
        "outputId": "625f8fef-98a6-4b57-ee1d-9d9f84e1513f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8612 entries, 0 to 8611\n",
            "Data columns (total 35 columns):\n",
            " #   Column                                                       Non-Null Count  Dtype \n",
            "---  ------                                                       --------------  ----- \n",
            " 0   TAG                                                          8612 non-null   object\n",
            " 1   SHORT                                                        8612 non-null   object\n",
            " 2   LONG                                                         8612 non-null   object\n",
            " 3   DISPLAY_ORDER                                                8612 non-null   int64 \n",
            " 4   PARENT_TAG                                                   8604 non-null   object\n",
            " 5   HIER_NUM                                                     8612 non-null   int64 \n",
            " 6   HIER_NAME                                                    8612 non-null   object\n",
            " 7   HIER_LEVEL_NUM                                               8612 non-null   int64 \n",
            " 8   HIER_LEVEL_NAME                                              8612 non-null   object\n",
            " 9   DISHWASHER PRODUCTS BY SECTOR OF FABRIC CARE level 0         2410 non-null   object\n",
            " 10  SECTOR OF FABRIC CARE                                        8161 non-null   object\n",
            " 11  TRADING COMPANY                                              8131 non-null   object\n",
            " 12  BRAND                                                        7279 non-null   object\n",
            " 13  PRODUCT PROPERTY   FORM                                      6694 non-null   object\n",
            " 14  WEIGHT VOLUME                                                2561 non-null   object\n",
            " 15  COLOUR FRAGRANCE OF HOUSEHOLD                                3809 non-null   object\n",
            " 16  BASE NUMBER IN MULTIPACK                                     2134 non-null   object\n",
            " 17  ALL SPECIAL OFFERS                                           1083 non-null   object\n",
            " 18  CUSTOM BRAND 1                                               330 non-null    object\n",
            " 19  DISHWASHER PRODUCTS BY TRADING COMPANY level 0               395 non-null    object\n",
            " 20  FRAGRANCE                                                    130 non-null    object\n",
            " 21  PRODUCT RANGE                                                2241 non-null   object\n",
            " 22  CUSTOM BRAND 2                                               267 non-null    object\n",
            " 23  DISHWASHER PRODUCTS BY PRODUCT PROPERTY FORM level 0         384 non-null    object\n",
            " 24  PACKAGING PROPERTY FORM                                      383 non-null    object\n",
            " 25  DISHWASHER PRODUCTS BY SECTOR OF FABRIC CARE #1 level 0      2188 non-null   object\n",
            " 26  NUMBER IN PACK                                               3596 non-null   object\n",
            " 27  CSA RECKITT BENCKISER SUB SC                                 1245 non-null   object\n",
            " 28  DISHWASHER PRODUCTS BY CSA RECKITT BENCKISER SUB SC level 0  1246 non-null   object\n",
            " 29  DISHWASHER PRODUCTS BY SECTOR BY PREMIUM level 0             1452 non-null   object\n",
            " 30  GLOBAL PRICE SEGMENT                                         1562 non-null   object\n",
            " 31  GLOBAL SIZE RANGE                                            1096 non-null   object\n",
            " 32  ADDITIVES BY TRADING COMPANY level 0                         211 non-null    object\n",
            " 33  DISHWASHER PRODUCTS BY SUMMARY SIZE BY SECTOR level 0        326 non-null    object\n",
            " 34  RB SUMMARY SIZE RANGE                                        318 non-null    object\n",
            "dtypes: int64(3), object(32)\n",
            "memory usage: 2.3+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annual_calculator_df = pd.DataFrame()  # Initialize an empty DataFrame to store the results\n",
        "def annual_calculator(concatenated_columns, df_PER):\n",
        "    # Step 1: Drop rows with null values\n",
        "    for col, df in concatenated_columns.items():\n",
        "        concatenated_columns[col] = df.dropna()\n",
        "\n",
        "    # Step 2: Merge concatenated_columns with df_PER using 'TAG' column for each column chunk\n",
        "        # Assuming 'PER_TAG' column in concatenated_columns is the same as df_PER['TAG']\n",
        "    merged_df = pd.merge(df, df_PER[['TAG', 'LONG']], left_on='PER_TAG', right_on='TAG', how='inner')\n",
        "\n",
        "        # Summing up values in case of multiple matches\n",
        "    merged_df = merged_df.groupby(['TAG', 'LONG']).sum().reset_index()\n",
        "\n",
        "        # Append the merged_df to annual_calculator_df\n",
        "    if annual_calculator_df.empty:\n",
        "            annual_calculator_df = merged_df.copy()\n",
        "    else:\n",
        "          annual_calculator_df = pd.concat([annual_calculator_df, sum_row.to_frame().T], ignore_index=True)\n",
        "\n",
        "    # Adding a row with the sum of all columns except the first one\n",
        "    sum_row = annual_calculator_df.iloc[:, 1:].sum()  # Calculate sum excluding the first column\n",
        "    sum_row['TAG'] = 'Sum'# Set 'Sum' as the TAG value for this row\n",
        "    sum_row['LONG'] = 'Sum'\n",
        "    annual_calculator_df = annual_calculator_df.append(sum_row, ignore_index=True)\n",
        "\n",
        "    return annual_calculator_df"
      ],
      "metadata": {
        "id": "5GLgdW7StSa9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_analysis_df = pd.DataFrame()  # Initialize an empty DataFrame to store the results\n",
        "def second_analysis(concatenated_columns, df_MKT):\n",
        "\n",
        "    # Step 1: Iterate through the column chunks and perform the required operations\n",
        "    for col, df_chunk in concatenated_columns.items():\n",
        "        # Drop rows with null values\n",
        "        df_chunk_cleaned = df_chunk.dropna()\n",
        "\n",
        "        # Merge concatenated column with df_MKT using 'MKT_TAG' and 'TAG' columns\n",
        "    merged_df = pd.merge(df_chunk_cleaned, df_MKT[['TAG', 'LONG']], left_on='MKT_TAG', right_on='TAG', how='inner')\n",
        "\n",
        "        # Summing up values in case of multiple matches\n",
        "    merged_df = merged_df.groupby(['TAG', 'LONG']).sum().reset_index()\n",
        "\n",
        "        # Append the merged_df to second_analysis_df\n",
        "    if second_analysis_df.empty:\n",
        "            second_analysis_df = merged_df.copy()\n",
        "    else:\n",
        "            second_analysis_df = pd.concat([second_analysis_df, merged_df], ignore_index=True)\n",
        "\n",
        "    return second_analysis_df\n",
        "\n",
        "# Example usage:\n",
        "# Assuming concatenated_columns is a dictionary containing column chunks\n",
        "# Assuming df_MKT is a DataFrame containing the 'TAG' and 'LONG' columns for merging\n",
        "\n",
        "# Call second_analysis function for concatenated columns and df_MKT\n",
        "# result_df = second_analysis(concatenated_columns, df_MKT)\n"
      ],
      "metadata": {
        "id": "vWBM0mR5_qrt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "third_analysis_df = pd.DataFrame()  # Initialize an empty DataFrame to store the results\n",
        "def third_analysis(concatenated_columns, df_PROD):\n",
        "\n",
        "    # Step 1: Iterate through the column chunks and perform the required operations\n",
        "    for col, df_chunk in concatenated_columns.items():\n",
        "        # Drop rows with null values\n",
        "        df_chunk_cleaned = df_chunk.dropna()\n",
        "\n",
        "        # Merge concatenated column with df_PROD using 'PROD_TAG' and 'TAG' columns\n",
        "    merged_df = pd.merge(df_chunk_cleaned, df_PROD[['TAG', 'LONG']], left_on='PROD_TAG', right_on='TAG', how='inner')\n",
        "\n",
        "        # Summing up values in case of multiple matches\n",
        "    merged_df = merged_df.groupby(['TAG', 'LONG']).sum().reset_index()\n",
        "\n",
        "        # Append the merged_df to third_analysis_df\n",
        "    if third_analysis_df.empty:\n",
        "            third_analysis_df = merged_df.copy()\n",
        "    else:\n",
        "            third_analysis_df = pd.concat([third_analysis_df, merged_df], ignore_index=True)\n",
        "\n",
        "    return third_analysis_df\n",
        "\n",
        "# Example usage:\n",
        "# Assuming concatenated_columns is a dictionary containing column chunks\n",
        "# Assuming df_PROD is a DataFrame containing the 'TAG' and 'LONG' columns for merging\n",
        "\n",
        "# Call third_analysis function for concatenated columns and df_PROD\n",
        "# result_df = third_analysis(concatenated_columns, df_PROD)\n"
      ],
      "metadata": {
        "id": "JZX7Lh0P_vdh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rPLPJUOWEZIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'annual_calculator_df' is your DataFrame with the 'Sum' row added\n",
        "# Find the maximum value in the 'Sum' row\n",
        "max_value = annual_calculator_df.iloc[-1, 1:].max()\n",
        "print(f\"The maximum value in the 'Sum' row is: {max_value}\")"
      ],
      "metadata": {
        "id": "yzSLsbXyHbzG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f1d56561-58d8-467d-9fc5-d41fd3ebd5d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eca53d51e6fc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming 'annual_calculator_df' is your DataFrame with the 'Sum' row added\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Find the maximum value in the 'Sum' row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannual_calculator_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The maximum value in the 'Sum' row is: {max_value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_tuple_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1464\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV file path\n",
        "csv_file_path = '/content/data/RB_DishwashWeekly_GB_fact_data_07072021.csv'\n",
        "\n",
        "# Define common columns\n",
        "common_columns = ['MKT_TAG', 'PROD_TAG', 'PER_TAG']\n",
        "\n",
        "# Read a small chunk to get the column names\n",
        "small_chunk = pd.read_csv(csv_file_path, delimiter='|', nrows=5)  # Adjust nrows based on your data to get a few rows for column names\n",
        "\n",
        "# Get the remaining columns excluding the common ones\n",
        "non_common_columns = [col for col in small_chunk.columns if col not in common_columns]\n",
        "\n",
        "# Set the chunk size to read all columns in each chunk\n",
        "chunk_size = 1000000  # Adjust chunk size as needed based on your system memory\n",
        "\n",
        "# DataFrame to store concatenated columns\n",
        "concatenated_columns = None\n",
        "# List to store results for each analysis function\n",
        "result1_list = []\n",
        "result2_list = []\n",
        "result3_list = []\n",
        "\n",
        "# Your loop to read non-common columns and concatenate chunks with common columns\n",
        "for col in non_common_columns:\n",
        "    print(f\"Processing column: {col}\")\n",
        "    reader = pd.read_csv(csv_file_path, delimiter='|', usecols=common_columns + [col], chunksize=chunk_size)\n",
        "\n",
        "    # List to store concatenated chunks for a specific column\n",
        "    concatenated_chunks = []\n",
        "\n",
        "    for i, df_chunk in enumerate(reader, 1):\n",
        "        print(f\"Chunk {i} - Column: {col}\")\n",
        "        # Concatenate specific column with common columns into a single DataFrame\n",
        "        concatenated_chunk = pd.concat([df_chunk[common_col] for common_col in common_columns] + [df_chunk[col]], axis=1)\n",
        "        concatenated_chunks.append(concatenated_chunk)  # Append the combined DataFrame\n",
        "\n",
        "    # Concatenate rows of the specific column into a single DataFrame\n",
        "    concatenated_columns_col = pd.concat(concatenated_chunks, ignore_index=True)\n",
        "    print(concatenated_columns_col.info())  # Display info for the concatenated DataFrame\n",
        "\n",
        "    # Concatenate across different columns\n",
        "    if concatenated_columns is None:\n",
        "        concatenated_columns = concatenated_columns_col\n",
        "    else:\n",
        "        concatenated_columns = pd.concat([concatenated_columns, concatenated_columns_col], axis=1)\n",
        "\n",
        "    # Perform analysis functions on the concatenated column\n",
        "    result1 = annual_calculator(concatenated_columns_col, df_PER)\n",
        "    result2 = second_analysis_function(concatenated_columns_col, df_MKT)\n",
        "    result3 = third_analysis_function(concatenated_columns_col, df_PROD)\n",
        "\n",
        "    # Append results to the lists\n",
        "    result1_list.append(result1)\n",
        "    result2_list.append(result2)\n",
        "    result3_list.append(result3)\n",
        "\n",
        "# Concatenate results across different columns\n",
        "final_result1 = pd.concat(result1_list)\n",
        "final_result2 = pd.concat(result2_list)\n",
        "final_result3 = pd.concat(result3_list)\n"
      ],
      "metadata": {
        "id": "7SzuJE3XPfMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2-binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DulCy2IMaJVY",
        "outputId": "091d9ba4-36d9-4ad8-f9fb-c6c38c756de6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "anothe approch is using postgre and use join statement to get different queries."
      ],
      "metadata": {
        "id": "13hcM-BF5FqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import psycopg2\n",
        "\n",
        "# CSV file path\n",
        "csv_file_path = '/content/data/RB_DishwashWeekly_GB_fact_data_07072021.csv'\n",
        "\n",
        "# Read a small chunk to get column names and datatypes\n",
        "chunksize = 1000  # Adjust the chunksize as needed\n",
        "csv_reader = pd.read_csv(csv_file_path, nrows=5)\n",
        "column_datatypes = {col: csv_reader[col].dtype for col in csv_reader.columns}\n",
        "\n",
        "# PostgreSQL connection details\n",
        "dbname = 'your_dbname'\n",
        "user = 'your_username'\n",
        "password = 'your_password'\n",
        "host = 'localhost'  # Change it to your PostgreSQL host\n",
        "port = '5432'       # Change it to your PostgreSQL port\n",
        "\n",
        "# Connect to PostgreSQL\n",
        "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)\n",
        "\n",
        "# Create a table in the database based on CSV columns and datatypes\n",
        "create_table_query = f\"CREATE TABLE IF NOT EXISTS your_table_name ({', '.join([f'{col} {column_datatypes[col]}' for col in column_datatypes.keys()])});\"\n",
        "with conn.cursor() as cursor:\n",
        "    cursor.execute(create_table_query)\n",
        "conn.commit()\n",
        "\n",
        "# Read and insert data into PostgreSQL\n",
        "csv_reader = pd.read_csv(csv_file_path, chunksize=100000)  # Adjust chunksize as needed\n",
        "for chunk in csv_reader:\n",
        "    chunk.to_sql('your_table_name', conn, if_exists='append', index=False)\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "681vYijr5B1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-sql\n",
        "!pip install psycopg2-binary\n"
      ],
      "metadata": {
        "id": "15EjfvvuA_D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext sql\n",
        "\n",
        "# PostgreSQL connection string\n",
        "connection_string = 'postgresql://username:password@host:port/database'\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "%sql $connection_string\n"
      ],
      "metadata": {
        "id": "l1FktH0cA_mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "-- Create a temporary view joining the fact table with the market table\n",
        "CREATE TEMP VIEW fact_market_view AS\n",
        "SELECT f.mkr_tag, f.*, m.*\n",
        "FROM fact_table f\n",
        "JOIN market_table m ON f.mkr_tag = m.tag;\n",
        "\n",
        "-- Sum the values of columns starting with 'F' for each market tag\n",
        "SELECT mkr_tag, SUM(CASE WHEN column_name LIKE 'F%' THEN column_name ELSE 0 END) AS sum_f_columns\n",
        "FROM fact_market_view\n",
        "GROUP BY mkr_tag\n",
        "ORDER BY sum_f_columns DESC;\n"
      ],
      "metadata": {
        "id": "sxMzbo6jBEZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Execute SQL query and store the result in a pandas DataFrame\n",
        "result = %sql SELECT mkr_tag, SUM(CASE WHEN column_name LIKE 'F%' THEN column_name ELSE 0 END) AS sum_f_columns FROM fact_market_view GROUP BY mkr_tag ORDER BY sum_f_columns DESC\n",
        "df = result.DataFrame()\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='mkr_tag', y='sum_f_columns', data=df)\n",
        "plt.title('Sum of Columns Starting with \"F\" for Each mkr_tag')\n",
        "plt.xlabel('mkr_tag')\n",
        "plt.ylabel('Sum of F Columns')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7wTsH5CyBJBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the relevant data from the PostgreSQL database into a pandas DataFrame.\n",
        "Sum the values in columns that start with 'F', treating null values as zero.\n",
        "Visualize the sums in descending order.\n",
        "Categorize the sums into quartiles and create a visualization based on these quartiles."
      ],
      "metadata": {
        "id": "-lu1uATMC9dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Execute SQL query to retrieve relevant data\n",
        "query = '''\n",
        "SELECT mkr_tag,\n",
        "       COALESCE(SUM(CASE WHEN column_name LIKE 'F%' THEN column_name ELSE 0 END), 0) AS sum_f_columns\n",
        "FROM fact_market_view\n",
        "GROUP BY mkr_tag\n",
        "ORDER BY sum_f_columns DESC\n",
        "'''\n",
        "result = %sql {query}\n",
        "df = result.DataFrame()\n",
        "\n",
        "# Sum values in 'F' columns, treating null values as zero\n",
        "f_columns = [col for col in df.columns if col.startswith('F')]\n",
        "df['sum_of_F'] = df[f_columns].fillna(0).sum(axis=1)\n",
        "\n",
        "# Visualize the sums in descending order\n",
        "df_sorted = df.sort_values('sum_of_F', ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df_sorted['mkr_tag'], df_sorted['sum_of_F'])\n",
        "plt.title('Sum of \"F\" Columns in Descending Order')\n",
        "plt.xlabel('mkr_tag')\n",
        "plt.ylabel('Sum of F Columns')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Categorize the sums into quartiles and create a visualization\n",
        "quartiles = pd.qcut(df['sum_of_F'], q=4)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=quartiles, y=df['sum_of_F'])\n",
        "plt.title('Sum of \"F\" Columns by Quartiles')\n",
        "plt.xlabel('Quartiles')\n",
        "plt.ylabel('Sum of F Columns')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w5bTI9RyBjK9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}